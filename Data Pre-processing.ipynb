{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Pre-processing",
   "id": "62d9901c78e9ebea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "import html\n",
    "import torch\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "22910a70f01479ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path = 'amazon_dataset.json'\n",
    "dataset = load_dataset('json', data_files=path)\n",
    "dataset"
   ],
   "id": "2b88c13d7de99b50",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset.__len__()",
   "id": "833ff109943479d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "raw_dataset = dataset['train']",
   "id": "bb6401646741117f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a function to concatenate title and text\n",
    "def combine_title_text(example):\n",
    "    # Concatenate title and text, separated by a space or any other delimiter if needed\n",
    "    example['reviews'] = example['title'] + \". \" + example['text']\n",
    "    return example\n",
    "\n",
    "# Apply the function to the dataset\n",
    "raw_dataset = raw_dataset.map(combine_title_text, num_proc=4)"
   ],
   "id": "9252b966c6a143d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Text Cleaning",
   "id": "bc1663c331c17088"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "emoticons_dict = {\n",
    "    \":*\": \"kiss\", \":-*\": \"kiss\", \":x\": \"kiss\", \":-)\": \"happy\", \":-))\": \"happy\",\n",
    "    \":-)))\": \"happy\", \":-))))\": \"happy\", \":-)))))\": \"happy\", \":-))))))\": \"happy\",\n",
    "    \":)\": \"happy\", \":))\": \"happy\", \":)))\": \"happy\", \":))))\": \"happy\", \":)))))\": \"happy\",\n",
    "    \":))))))\": \"happy\", \":)))))))\": \"happy\", \":o)\": \"happy\", \":]\": \"happy\", \":3\": \"happy\",\n",
    "    \":c)\": \"happy\", \":>\": \"happy\", \"=]\": \"happy\", \"8)\": \"happy\", \"=)\": \"happy\", \":}\": \"happy\",\n",
    "    \":^)\": \"happy\", \"|;-)\": \"happy\", \":'-)\": \"happy\", \":')\": \"happy\", \"\\\\o/\": \"happy\",\n",
    "    \"*\\\\0/*\": \"happy\", \":-D\": \"laugh\", \":D\": \"laugh\", \"8-D\": \"laugh\", \"8D\": \"laugh\",\n",
    "    \"x-D\": \"laugh\", \"xD\": \"laugh\", \"X-D\": \"laugh\", \"XD\": \"laugh\", \"=-D\": \"laugh\", \"=D\": \"laugh\",\n",
    "    \"=-3\": \"laugh\", \"=3\": \"laugh\", \"B^D\": \"laugh\", \">:[\" : \"sad\", \":-(\": \"sad\", \":-(((\": \"sad\",\n",
    "    \":(\": \"sad\", \":))\": \"happy\", \";)\": \"wink\", \":-P\": \"tong\", \">:\\\\\": \"annoyed\", \":-|\": \"annoyed\",\n",
    "    \"<3\": \"heart\", \"o_O\": \"surprise\", \">:)\": \"devil\", \"D:<\": \"sad\", \":-#\": \"seallips\", \"O:-)\": \"angel\"\n",
    "}"
   ],
   "id": "463102aacb44f8f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import emoji\n",
    "# Function to convert emojis to text\n",
    "def convert_emojis_to_text(text):\n",
    "\n",
    "    for emoticon, description in emoticons_dict.items():\n",
    "        text = text.replace(emoticon, description)\n",
    "    return emoji.demojize(text)"
   ],
   "id": "ad32cbba923e8c22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from nltk.corpus import wordnet     # Install using `pip install nltk`\n",
    "import nltk\n",
    "nltk.download(\"all\")"
   ],
   "id": "b9f75446fc233c73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "# Fix randomness in language detection for consistency\n",
    "DetectorFactory.seed = 42\n",
    "\n",
    "# Define a function to detect and filter English text\n",
    "def is_english(review):\n",
    "    try:\n",
    "        return detect(review['title']) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# raw_dataset = raw_dataset.filter(is_english, num_proc=4)"
   ],
   "id": "82a9dee76a1aae20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Trim leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    text = re.sub(r'\\[\\[.*?\\]\\]', '', text)\n",
    "    text = text.replace('\\/', '/')\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    text = re.sub(r'\\\\u2019', \"'\", text)\n",
    "    text = re.sub(r'\\\\u201c', '\"', text)\n",
    "    text = re.sub(r'\\\\u201d', '\"', text)\n",
    "    text = re.sub(r\"\\\\u2013\", \"-\", text)\n",
    "    text = re.sub(r'\\\\u200d', '', text)\n",
    "    pattern = rf'([{re.escape(string.punctuation)}])\\1+'\n",
    "    text = re.sub(pattern, r'\\1', text)\n",
    "\n",
    "    text = html.unescape(text)\n",
    "\n",
    "\n",
    "    # Remove extra whitespaces\n",
    "\n",
    "    text = convert_emojis_to_text(text)\n",
    "    # text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "raw_dataset = raw_dataset.map(lambda x: {'cleaned_reviews': clean_text(x['reviews'])}, num_proc=4)"
   ],
   "id": "108a17855f5fac88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Sample text cleaning\n",
    "a = '''In my search to find a more affordable option to more expensive peppermint oil supplements, \n",
    "I ordered this.  Opened it and the rancid oil smell was overwhelming. Not subtle... it was nauseating.  So disappointed.  It does contain sunflower seed oil.  IBgard is expensive and contains artificial dyes.  So I was hopeful for this, but my search continues.  These pills are much better sized than the more expensive brand, but again, the rancid oil smell stopped me from even considering taking it. For those interested, I checked the expiration date and it shows 4/19.  This is my first order of this particular brand of supplements.<br />Here is the ingredient list<br />Oil of peppermint  50mg<br />&#34;Other ingredients&#34;  Sunflower seed oil, gelatin, vegetable glycerin, food glaze. Contains less than 2% acetlylated monoglycerides, polysorbate 80, sodium alginate, sorbic acid (preservative) and purified water.<br />These are not for me.  I will be tossing them toot sweet.  I update my reviews all the time if new info would helpful. \n",
    "And I DO NOT receive compensation of any kind for my reviews.\n",
    "'''\n",
    "print(clean_text(a))"
   ],
   "id": "9fa5dbd2b29701e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "raw_dataset",
   "id": "127a773877f665f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter Reviews",
   "id": "d8e5499de7df6292"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "MIN_WORD_COUNT = 10\n",
    "# Define a function to check if a review meets the minimum word count\n",
    "def is_long_enough(review):\n",
    "    return len(review.split()) >= MIN_WORD_COUNT\n",
    "\n",
    "# Apply the filter to keep only reviews with the minimum word count\n",
    "raw_dataset = raw_dataset.filter(lambda x: is_long_enough(x[\"cleaned_reviews\"]), num_proc=4)"
   ],
   "id": "e58bd598750acb9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import concatenate_datasets\n",
    "\n",
    "# Function to filter, sample, and map float labels to integer classes\n",
    "def filter_sample_and_map_labels(dataset, rating_value, label_int, sample_size=100000):\n",
    "    filtered_dataset = dataset.filter(lambda x: x['rating'] == float(rating_value), num_proc=4)\n",
    "    sampled_dataset = filtered_dataset.shuffle(seed=42).select(range(sample_size))\n",
    "    return sampled_dataset.map(lambda x: {'target': label_int}, num_proc=4)\n",
    "\n",
    "# Apply function to get 45,000 samples each for ratings 1.0, 3.0, and 5.0\n",
    "rating_1_data = filter_sample_and_map_labels(raw_dataset, 1.0, 0)\n",
    "#rating_2_data = filter_sample_and_map_labels(raw_dataset, 2.0, 2)\n",
    "rating_3_data = filter_sample_and_map_labels(raw_dataset, 3.0, 1)\n",
    "#rating_4_data = filter_sample_and_map_labels(raw_dataset, 4.0, 4)\n",
    "rating_5_data = filter_sample_and_map_labels(raw_dataset, 5.0, 2)\n",
    "\n",
    "# Concatenate the sampled datasets\n",
    "sampled_dataset = concatenate_datasets([rating_1_data, rating_3_data, rating_5_data]).shuffle(seed=42)"
   ],
   "id": "39549f7bedee4bcf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "num_classes = 3  # Adjust this to the actual number of classes\n",
    "class_label = ClassLabel(num_classes=num_classes)\n",
    "\n",
    "# Map the target column to ClassLabel type\n",
    "sampled_dt = sampled_dataset.cast_column(\"target\", class_label)\n",
    "\n",
    "train_val_dataset = sampled_dt.train_test_split(test_size=0.3, seed=42, stratify_by_column=\"target\")\n",
    "train_dataset = train_val_dataset['train']\n",
    "temp_dataset = train_val_dataset['test']\n",
    "validation_test_dataset = temp_dataset.train_test_split(test_size=0.5, seed=42, stratify_by_column=\"target\")\n",
    "\n",
    "validation_dataset = validation_test_dataset['train']\n",
    "test_dataset = validation_test_dataset['test']"
   ],
   "id": "757c38148b486ae5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation",
   "id": "eba6b379121f2fae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nlpaug.augmenter.word as naw\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "\n",
    "# Initialize augmenters\n",
    "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "\n",
    "def create_back_translation_model(device):\n",
    "    \"\"\"Create a BackTranslationAug instance for a specific device.\"\"\"\n",
    "    return naw.BackTranslationAug(\n",
    "        from_model_name='Helsinki-NLP/opus-mt-en-de', #using smaller model for GPU compatibility\n",
    "        to_model_name='Helsinki-NLP/opus-mt-de-en',\n",
    "        device=device\n",
    "    )\n",
    "'''Large model for better results :\n",
    "    from_model_name='facebook/wmt19-en-de',  \n",
    "    to_model_name='facebook/wmt19-de-en',\n",
    "    '''\n",
    "# contextual_word_embs_aug = naw.ContextualWordEmbsAug(\n",
    "#     model_path='bert-base-uncased', action=\"insert\", device='cuda'\n",
    "# )\n",
    "\n",
    "\n",
    "def batch_synonym_augmentation(examples):\n",
    "    augmented_texts = synonym_aug.augment(examples['cleaned_reviews'])\n",
    "    return {'cleaned_reviews': augmented_texts, 'augmented': [True] * len(augmented_texts)}\n",
    "\n",
    "# def batch_back_translation(examples):\n",
    "#     with autocast():\n",
    "#         augmented_texts = back_translation_aug.augment(examples['cleaned_reviews'])\n",
    "#     return {'cleaned_reviews': augmented_texts, 'augmented': [True] * len(augmented_texts)}\n",
    "# \n"
   ],
   "id": "64f2616a78d3932"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define augmentation ratio and parallel processing parameters\n",
    "import random\n",
    "augmentation_ratio = 0.4  # Apply each augmentation to 40% of the dataset\n",
    "num_proc = 4  # Use 4 processes in parallel\n",
    "# Apply batch processing for back-translation with ratio\n",
    "def apply_batch_augmentation_with_ratio(dataset, aug_func, batch_size=64):\n",
    "    # Apply augmentation only to the specified ratio of the dataset\n",
    "    augmented_dataset = dataset.filter(lambda x: random.random() <= augmentation_ratio)\n",
    "\n",
    "    # Apply back-translation in batches\n",
    "    augmented_dataset = augmented_dataset.map(\n",
    "        aug_func,\n",
    "        batched=True,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    return augmented_dataset"
   ],
   "id": "ce33fb3bbd97f770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# for i in range(21):\n",
    "#     print(synonym_augmentation(raw_dataset['cleaned_reviews'][i]), end=\"\\n\\n\")\n",
    "#     print('type', type(raw_dataset['cleaned_reviews'][i]) )\n",
    "# Select the first 5 rows\n",
    "#subset = raw_dataset.select(range(5))\n",
    "\n",
    "# Apply `map` to only this subset\n",
    "# augmented_subset = subset.map(\n",
    "#     lambda x: apply_augmentation_with_ratio(x, synonym_augmentation)\n",
    "# )\n"
   ],
   "id": "acd4b29c2fb32432"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# # Assuming raw_dataset is a `datasets.Dataset` object\n",
    "# def extract_rating(example):\n",
    "#     return {'rating': example['rating']}\n",
    "\n",
    "# # Map to extract ratings\n",
    "# ratings_dataset = raw_dataset.map(extract_rating, remove_columns=raw_dataset.column_names, num_proc=4)\n",
    "\n",
    "# # Convert ratings to a list and count them\n",
    "# ratings = ratings_dataset['rating']\n",
    "# rating_counter = Counter(ratings)\n",
    "\n",
    "\n",
    "# print(rating_counter)"
   ],
   "id": "d2e9489c18ae6e88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if torch.cuda.is_available():\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(f\"Number of available GPUs: {num_devices}\")\n",
    "    for i in range(num_devices):\n",
    "        device_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"Device {i}: {device_name}\")\n",
    "else:\n",
    "    print(\"No CUDA GPUs found.\")"
   ],
   "id": "27ad60799cd3701c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Data Augmentation- Back Translation",
   "id": "ce4e7d1585c2c2a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import random\n",
    "from datasets import Dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "import torch\n",
    "from accelerate.utils import gather_object\n",
    "\n",
    "\n",
    "os.environ[\"NCCL_TIMEOUT\"] = \"1800\"\n",
    "# Add an empty 'corrected_sentence' column\n",
    "dataset_1 = train_dataset#.select(range(100))\n",
    "print('Dataset len: ',len(dataset_1))\n",
    "\n",
    "# Apply augmentation to a random subset of the dataset\n",
    "augmentation_ratio = 0.4  # Apply augmentation to 40% of the dataset\n",
    "\n",
    "def filter_function(example):\n",
    "    return random.random() <= augmentation_ratio\n",
    "\n",
    "# Filter the dataset for augmentation\n",
    "filtered_dataset = dataset_1.filter(filter_function)\n",
    "print('Filtered Dataset len: ',len(filtered_dataset))\n",
    "\n",
    "\n",
    "\n",
    "def back_translation_multi_gpus():\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    back_translation_aug = create_back_translation_model(device)\n",
    "\n",
    "    filtered_dataset_1 = filtered_dataset.add_column(\"global_index\", list(range(len(filtered_dataset))))\n",
    "    # filtered_dataset_1 = filtered_dataset_1.add_column(\"corrected_sentence\", filtered_dataset_1[\"cleaned_reviews\"])\n",
    "    # print(\"'corrected_sentence' column initialized with 'cleaned_reviews'.\")\n",
    "\n",
    "    # Set the format for PyTorch\n",
    "    filtered_dataset_1.set_format(type='torch', columns=['cleaned_reviews', 'global_index'])\n",
    "    # Create DataLoader\n",
    "    batch_size = 40\n",
    "    dataloader = DataLoader(filtered_dataset_1, batch_size=batch_size)\n",
    "    dataloader = accelerator.prepare(dataloader)\n",
    "\n",
    "\n",
    "    indexed_corrected_sentences=[]\n",
    "\n",
    "\n",
    "    # No gradient calculation needed during inference\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Running BackTranslation\"):\n",
    "            torch.cuda.empty_cache()\n",
    "            indices = batch['global_index'].tolist()\n",
    "            cleaned_reviews = batch['cleaned_reviews']\n",
    "            augmented_texts = back_translation_aug.augment(cleaned_reviews)\n",
    "            if len(augmented_texts) != len(cleaned_reviews):\n",
    "                print(f\"Warning: Expected {len(cleaned_reviews)} augmented sentences, but got {len(augmented_texts)}\")\n",
    "                # Handle missing augmentations by retaining original sentences\n",
    "                # This ensures that row counts remain consistent\n",
    "                if len(augmented_texts) < len(cleaned_reviews):\n",
    "                    augmented_texts += cleaned_reviews[len(augmented_texts):]\n",
    "            # Collect augmented sentences and indices\n",
    "            indexed_corrected_sentences.extend(zip(indices, augmented_texts))\n",
    "\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    print('Gathering....')\n",
    "    gathered_results = gather_object(indexed_corrected_sentences)\n",
    "\n",
    "    # Ensure the results are only collected once\n",
    "    if accelerator.is_main_process:\n",
    "        print(\"Processing final results on the main process...\")\n",
    "        # index_to_sentence = dict(zip(gathered_results[0], gathered_results[1]))\n",
    "        output_dict = dict(gathered_results)\n",
    "        gathered_indices = set(output_dict.keys())\n",
    "        sorted_items = sorted(output_dict.items())\n",
    "        print('gathered_results',len(gathered_results))\n",
    "\n",
    "        # Update the original dataset with augmented sentences\n",
    "        augmented_cleaned_reviews = [sentence for _, sentence in sorted_items]\n",
    "        print('corrected_sentences',len(augmented_cleaned_reviews))\n",
    "        filtered_dataset_1.reset_format()\n",
    "\n",
    "        filtered_dataset_1 = filtered_dataset_1.add_column(\"augmented_cleaned_reviews\", augmented_cleaned_reviews)\n",
    "        print('type', type(filtered_dataset_1))\n",
    "\n",
    "\n",
    "        # Save the dataset to disk\n",
    "        filtered_dataset_1.save_to_disk('/data/augmented_back_translation')\n",
    "        print(\"Augmented dataset saved to '/data/augmented_back_translation'.\")\n",
    "    \n",
    "    "
   ],
   "id": "274ba91acbae3331"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if __name__ == \"__main__\":\n",
    "    notebook_launcher(back_translation_multi_gpus, args=(), num_processes=torch.cuda.device_count())  # Adjust automatically based on available GPUs"
   ],
   "id": "ec33d5134c0b2ee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from datasets import load_from_disk\n",
    "# aug_t_dataset = load_from_disk('/kaggle/working/augmented_dataset').map(lambda x: x, keep_in_memory=True)\n"
   ],
   "id": "5535b49fb0a30dc2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Apply Data Augmentation- Synonym Replacement",
   "id": "4a4f8a97844bc8ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "%%time\n",
    "augmented_synonym = apply_batch_augmentation_with_ratio(train_dataset, batch_synonym_augmentation)\n",
    "# augmented_back_translation = apply_batch_augmentation_with_ratio(train_dataset, batch_back_translation)"
   ],
   "id": "3097e7c636e31c02"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# augmented_dataset = concatenate_datasets([sampled_dataset, augmented_synonym, augmented_back_translation]).shuffle(seed=42)",
   "id": "7a073cb4fd418e38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Save Datasets",
   "id": "c0d0c9d52a74944b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# augmented_back_translation_path = '/data/augmented_back_translation'\n",
    "# augmented_back_translation.save_to_disk(augmented_back_translation_path)\n",
    "augmented_synonym_path = '/data/augmented_synonym'\n",
    "augmented_synonym.save_to_disk(augmented_synonym_path)"
   ],
   "id": "c9d84893db76738"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_path = '/data/amazon_train_dataset'\n",
    "train_dataset.save_to_disk(train_path)"
   ],
   "id": "b6abe844450d568c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "test_path = '/data/amazon_test_dataset'\n",
    "test_dataset.save_to_disk(test_path)"
   ],
   "id": "fbe4e78dec64c536"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_path = '/data/amazon_validation_dataset'\n",
    "validation_dataset.save_to_disk(val_path)"
   ],
   "id": "b2e54744c773bf16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!zip -r augmented_split_dataset.zip \"$train_path\" \"$test_path\" \"$val_path\" \"$augmented_synonym_path\" \n",
    "#\"$augmented_back_translation_path\""
   ],
   "id": "b8d52a5821190ef3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
